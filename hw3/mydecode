#!/usr/bin/env python
import argparse
import sys
import mymodels
import heapq
import math
from collections import namedtuple

parser = argparse.ArgumentParser(description='Simple phrase based decoder.')
parser.add_argument('-i', '--input', dest='input', default='data/input', help='File containing sentences to translate (default=data/input)')
parser.add_argument('-t', '--translation-model', dest='tm', default='data/tm', help='File containing translation model (default=data/tm)')
parser.add_argument('-s', '--stack-size', dest='s', default=1, type=int, help='Maximum stack size (default=1)')
parser.add_argument('-d', '--reordering-distance', dest='d', default=2, type=int, help='Maximum reordering distance (default=2)')
parser.add_argument('-a', '--alpha', dest='a', default=0.5, type=float, help='Reordering penalty alpha (default=0.5)')
parser.add_argument('-n', '--num_sentences', dest='num_sents', default=sys.maxint, type=int, help='Number of sentences to decode (default=no limit)')
parser.add_argument('-l', '--language-model', dest='lm', default='data/lm', help='File containing ARPA-format language model (default=data/lm)')
parser.add_argument('-v', '--verbose', dest='verbose', action='store_true', default=False,  help='Verbose mode (default=off)')
opts = parser.parse_args()

tm = mymodels.TM(opts.tm, sys.maxint)
lm = mymodels.LM(opts.lm)
sys.stderr.write('Decoding %s...\n' % (opts.input,))
input_sents = [tuple(line.strip().split()) for line in open(opts.input).readlines()[:opts.num_sents]]

tmw = 1.0
lmw = 1.0
fcw = 1.0
rcw = 1.0

# Get the spans of untranslated input text
def get_spans(words):
    if len(words) == 0:
        return []
    if len(words) == 1:
        return [(words[0], words[0]+1)]

    words = sorted(words)
    spans = []

    start = words[0]
    for i, word in enumerate(words[1:]):
        if word != words[i]+1:
            spans.append((start, words[i]+1))
            start = word
        elif i == len(words)-2:
            spans.append((start, word))
    return spans
        
    

hypothesis = namedtuple('hypothesis', 'logprob, lm_state, predecessor, phrase, untranslated, end, dist, dp')
for f in input_sents:
    # The following code implements a DP monotone decoding
    # algorithm (one that doesn't permute the target phrases).
    # Hence all hypotheses in stacks[i] represent translations of 
    # the first i words of the input sentence.
    # HINT: Generalize this so that stacks[i] contains translations
    # of any i words (remember to keep track of which words those
    # are, and to estimate future costs)
    initial_hypothesis = hypothesis(0.0, lm.begin(), None, None, range(len(f)), 0.0, 0.0, 0.0)

    # Calcuate future costs
    fc = {}
    for length in range(1, len(f)+1):
        for start in range(len(f)+1-length):
            end = start+length
            fc[start, end] = float("-inf")
            if f[start:end] in tm:
                for phrase in tm[f[start:end]]:
                    if phrase.logprob > fc[start, end]:
                        fc[start, end] = phrase.logprob
            for i in range(start+1, end):
                if fc[start, i]+fc[i, end] > fc[start, end]:
                    fc[start, end] = fc[start, i]+fc[i, end]
    #print sorted(fc.items(), key=lambda x: x[1])

    stacks = [{} for _ in f] + [{}]
    stacks[0][lm.begin()] = initial_hypothesis
    for i, stack in enumerate(stacks[:-1]):
        # extend the top s hypotheses in the current stack
        for h in heapq.nlargest(opts.s, stack.itervalues(), key=lambda h: h.logprob): # prune
            for (start, length) in get_spans(h.untranslated):
                for end in xrange(start+1,length+1):
                    for k in xrange(0, opts.d+1):
                        if min(h.untranslated) < start:
                            continue
                        if end+k > length:
                            continue
                        if f[start+k:end+k] in tm:
                            for phrase in tm[f[start+k:end+k]]:
                                # Get future cost
                                untranslated = list(set(h.untranslated)-set(range(start+k, end+k)))
                                future_cost = 0.0
                                for span in get_spans(untranslated):
                                    future_cost += fc[span]

                                # Get reordering cost
                                x = start+k - h.end
                                reordering_cost = math.log(math.pow(opts.a, abs(x)))

                                lm_state = h.lm_state
                                lm_prob = 0.0
                                for word in phrase.english.split():
                                    (lm_state, word_logprob) = lm.score(lm_state, word)
                                    lm_prob += word_logprob
                                lm_prob += lm.end(lm_state) if end+k == len(f) else 0.0
                                
                                logprob = h.logprob + tmw*phrase.logprob + fcw*future_cost + rcw*reordering_cost + lmw*lm_prob

                                new_hypothesis = hypothesis(logprob, lm_state, h, phrase, untranslated, end+k, x, reordering_cost)
                                stack_num = len(f)-len(untranslated)
                                if lm_state not in stacks[stack_num]: # second case is recombination
                                    stacks[stack_num][lm_state] = new_hypothesis
                                elif stacks[stack_num][lm_state].logprob < logprob and len(set(stacks[stack_num][lm_state].untranslated) - set(untranslated)) == 0:
                                    stacks[stack_num][lm_state] = new_hypothesis
                            

    s_out = open("states.out", "wb")
    for i, stack in enumerate(stacks):
        s_out.write(str(i)+":\n")
        for lm_state, hyp in sorted(stack.iteritems(), key=lambda x: x[1].logprob, reverse=True):
            s_out.write(str(lm_state)+" : ")
            if hyp.predecessor != None:
                s_out.write(str(hyp.predecessor.lm_state)+" "+str(hyp.logprob)+" "+str(hyp.untranslated)+" "+str(hyp.dist)+" "+str(hyp.dp)+"\n")
    s_out.close()

    # find best translation by looking at the best scoring hypothesis
    # on the last stack
    winner = max(stacks[-1].itervalues(), key=lambda h: h.logprob)
    
    def extract_english_recursive(h):
        return '' if h.predecessor is None else '%s%s ' % (extract_english_recursive(h.predecessor), h.phrase.english)
    print extract_english_recursive(winner)

    if opts.verbose:
        def extract_tm_logprob(h):
            return 0.0 if h.predecessor is None else h.phrase.logprob + extract_tm_logprob(h.predecessor)
        tm_logprob = extract_tm_logprob(winner)
        sys.stderr.write('LM = %f, TM = %f, Total = %f\n' % 
            (winner.logprob - tm_logprob, tm_logprob, winner.logprob))
